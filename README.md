# Retrieval-Augmented Generation Chat Application âœ¨

A project demonstrating RAG capabilities using **Ollama** and **Wikipedia** as the knowledge base. This application shows how you can "finetune" any AI to **YOUR actual needs** without expensive model training. Simply feed your AI with your own knowledge base, company documents, internal wikis, product specifications, or any domain-specific content.

**The best part?** This architecture is easily applicable to other LLM models (GPT, Claude, etc.) and can work with your personal data sources beyond Wikipedia. The AI answers questions using your actual data, with full citation and source transparency.

---

## Why RAG?

Traditional AI models are trained on general knowledge and may not have access to your company's specific information, internal processes, or proprietary data. **Retrieval-Augmented Generation (RAG)** solves this by:

**No Model Training Required** - Use any pre-trained LLM (like Llama, GPT, Claude, etc.) without expensive fine-tuning  
**Real-Time Knowledge Updates** - Add new information instantly without retraining models  
**Source Transparency** - Every answer includes citations, so you know exactly where the information came from  
**Domain-Specific Expertise** - Transform generic AI into a specialist for your company's needs  
**Cost-Effective** - Significantly cheaper than training custom models while achieving similar results

### RAG in Action: Context Makes the Difference ğŸ¯

See the dramatic difference RAG makes when answering questions. The comparison below shows how context transforms generic AI responses into accurate, cited answers.

#### Without Context (Standard AI)

![No Context](demo/no_contex.png)

#### With RAG Context (Enhanced AI)

![With Context](demo/with_context.png)

**Without RAG**, the AI relies solely on its training data, which can lead to:
- Generic or outdated information
- No access to company-specific processes or documents
- Inability to cite sources
- Potential hallucinations when asked about proprietary information

**With RAG**, the AI:
- **Retrieves relevant chunks** ğŸ” from your knowledge base using semantic search
- **Synthesizes answers** ğŸ§  using both the retrieved context and its general knowledge
- **Provides citations** ğŸ“– linking back to the original sources
- **Stays current** âš¡ with your latest documents and information
- **Answers accurately** âœ… about your company's specific content

The difference is clear: RAG-powered responses are grounded in your actual data, making the AI a reliable assistant for your organization's needs.

### Knowledge Base Management ğŸ“š

Build and manage your knowledge base with ease. The interface below shows both ingestion and management capabilities side by side.

<table>
<tr>
<td width="50%">

**Populate Your Knowledge Base**

![Populate Knowledge Base](demo/Populate_Knowledge_Base.png)

</td>
<td width="50%">

**Manage Your References**

![Knowledge References](demo/knowlage_references.png)

</td>
</tr>
</table>

#### Populate Your Knowledge Base ğŸ¨

Easily add content to your knowledge base through two methods:

1. **Search by Topic** - Enter topics (one per line), and the system automatically fetches up to five relevant Wikipedia pages for each topic. Perfect for quickly building a knowledge base around specific subjects.

2. **Ingest Specific Articles** - Paste full Wikipedia URLs to embed exact articles. Ideal for curated reading lists or when you need precise control over the content.

The ingestion process:
- Fetches content from Wikipedia
- Splits documents into semantic chunks
- Generates vector embeddings for each chunk
- Stores everything in Qdrant vector database for fast retrieval

#### Manage Your References ğŸ—‚ï¸

Full transparency and control over your knowledge base:

- **View All Ingested Content** ğŸ‘€ - See every article, document, or reference in your knowledge base
- **Monitor Chunk Counts** ğŸ“Š - Understand how your content is structured and indexed
- **Remove Outdated Content** ğŸ—‘ï¸ - Delete references that are no longer relevant
- **Track Sources** ğŸ”— - Each reference shows title, topic, URL, and chunk count

This management interface ensures your knowledge base stays clean, relevant, and up-to-date with your company's evolving needs.

### Key Features âœ¨

- **Semantic Search** ğŸ” - Find relevant information using vector similarity search
- **Multi-Session Chat** ğŸ’¬ - Manage multiple conversation threads
- **Source Citations** ğŸ“ - Every answer includes clickable source links
- **Wikipedia Integration** ğŸŒ - Easy ingestion of Wikipedia content (easily extensible to other sources)
- **Real-Time Updates** âš¡ - Add or remove knowledge without downtime
- **Modern UI** ğŸ¨ - Clean, responsive React interface
- **RESTful API** ğŸ”Œ - Full FastAPI backend with OpenAPI documentation

---

## Setup & Installation

### Prerequisites

Before you begin, ensure you have the following installed:

- **Python 3.11+** ğŸ (with pip)
- **Node.js 18+** ğŸ“¦ and **npm**
- **Docker Desktop** ğŸ³ (for Qdrant and MongoDB)
- **Ollama** ğŸ¤– (for running local LLM models)

### Quick Start

The easiest way to get started is using our automated startup scripts:

#### Windows (PowerShell - Recommended)

```powershell
.\scripts\start.ps1
```

#### Windows (Command Prompt)

```cmd
scripts\start.bat
```

These scripts will:
1. Check if Docker is running âœ…
2. Start Qdrant vector database container (port 6333) ğŸ—„ï¸
3. Start MongoDB container (port 27017) ğŸƒ
4. Launch FastAPI backend server (port 8000) ğŸš€
5. Launch React frontend dev server (port 5173) âš›ï¸

To stop all services:

```powershell
.\scripts\stop.ps1
```

or

```cmd
scripts\stop.bat
```

### Manual Setup ğŸ”§

If you prefer to set up manually or need more control:

#### 1. Backend Setup

```bash
# Navigate to backend directory
cd backend

# Create and activate virtual environment (using conda as example)
conda create -n RAG python=3.11
conda activate RAG

# Install Python dependencies
pip install -r requirements.txt
```

#### 2. Frontend Setup

```bash
# Navigate to web directory
cd web

# Install Node.js dependencies
npm install
```

#### 3. Docker Services

Start Qdrant and MongoDB using Docker:

```bash
# Start Qdrant vector database
docker run -d \
  --name qdrant-rag \
  -p 6333:6333 \
  -p 6334:6334 \
  -v $(pwd)/qdrant_storage:/qdrant/storage \
  qdrant/qdrant

# Start MongoDB
docker run -d \
  --name mongo-rag \
  -p 27017:27017 \
  -v $(pwd)/mongo_data:/data/db \
  mongo:latest
```

#### 4. Environment Configuration

Create a `.env` file in the `backend/` directory (optional, defaults are provided):

```env
# MongoDB Configuration
MONGODB_URI=mongodb://localhost:27017
MONGODB_DATABASE=rag_portfolio

# Qdrant Configuration
QDRANT_URL=http://localhost:6333
COLLECTION_NAME=wiki_rag

# Embedding Model
EMBED_MODEL=sentence-transformers/bge-small-en-v1.5
VECTOR_SIZE=384

# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b

# Optional: Retriever threshold
RETRIEVER_SCORE_THRESHOLD=0.5
```

#### 5. Start Ollama

Make sure Ollama is installed and running with your chosen model:

```bash
# Install Ollama from https://ollama.ai
# Pull the model
ollama pull llama3.2:3b

# Start Ollama server (usually runs automatically)
```

#### 6. Run the Application

**Backend:**
```bash
cd backend
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

**Frontend:**
```bash
cd web
npm run dev
```

### Access Points ğŸŒ

Once everything is running, you can access:

- **Frontend UI** ğŸ–¥ï¸: http://localhost:5173
- **Backend API** ğŸ”Œ: http://localhost:8000
- **API Documentation** ğŸ“–: http://localhost:8000/docs
- **Qdrant Dashboard** ğŸ“Š: http://localhost:6333/dashboard
- **MongoDB** ğŸƒ: mongodb://localhost:27017

### Project Structure ğŸ“

```
Retrieval-Augmented-Generation/
â”œâ”€â”€ backend/                 # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ core/           # Settings and configuration
â”‚   â”‚   â”œâ”€â”€ db/             # Database clients (MongoDB, Qdrant)
â”‚   â”‚   â”œâ”€â”€ embeddings/     # Embedding model management
â”‚   â”‚   â”œâ”€â”€ models/         # Pydantic models
â”‚   â”‚   â”œâ”€â”€ routers/        # API route handlers
â”‚   â”‚   â””â”€â”€ services/       # Business logic services
â”‚   â””â”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ web/                     # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ api/            # API client
â”‚   â”‚   â”œâ”€â”€ components/     # React components
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ package.json        # Node.js dependencies
â”œâ”€â”€ scripts/                 # Startup/shutdown scripts
â”œâ”€â”€ demo/                    # Demo screenshots
â””â”€â”€ README.md               # This file
```

